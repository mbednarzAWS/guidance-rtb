# Guidance for Building a Real Time Bidder for Advertising on AWS

### RTB Code Kit Deployment Guide
# Guidance for Building a Real Time Bidder for Advertising on AWS

### RTB Code Kit Deployment Guide

## Introduction

Real-time advertising platforms operate round-the-clock, processing millions of transactions, running bidding, ad serving, and verification workloads at ultra-low latency and high throughput. 

In this industry, expenses associated with infrastructure costs such as compute, databases, and networking significantly impact profit margins. Consequently, ad tech firms are driven to constantly maximize the price-performance of their platforms.

This AWS Solution, "Guidance for Building a Real Time Bidder for Advertising on AWS" presents a deployable reference architecture that leverages open-source technologies to showcase the "art of the possible" in the advertising space. This blueprint empowers demand-side platforms (DSP) to develop advanced, innovative, and smart bidding services. 

The Real-Time Bidder Solution on AWS consists of 5 modules: 

1. **Data generator**: Generates synthetic data for the device, campaign, budget, and audience. The size of each table can be defined in the configuration file, it is recommended that one billion devices be generated for the devices table, one million campaigns and associated budgets, and one hundred thousand audiences.

2. **Load generator**: Generates artificial bid requests (based on the OpenRTB 2.5 or OpenRTB 3.0 standard).

3. **Bidder**: Receives and parses bid requests, searches for the device and associated metadata in the device table, selects the best fitting campaign, determines the bid price for the ad opportunity, constructs the bid request/response, and writes the bid request/response to a data pipeline. 

4. **Data repository**: Database for storing device, campaign, budget, and audience data. Supports either DynamoDB or Aerospike.

5. **Data Pipeline**: Receives, aggregates and writes the bid request and response to a data repository. Includes tools to ingest the metrics generated by the bidder and displays the results for evaluation.

6. **Monitoring Tools**: Uses Grafana and Prometheus on Amazon Elastic Kubernetes Service (EKS) to collect and display application logs such as the bid requests per second and latency.

## Prerequisites

1. Python 3

2. Ensure the AWS CLI and AWS CDK are installed on your local machine. These are used to access the EKS cluster and Grafana Dashboards:
    * [AWS CLI V2](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
    * [AWS CDK](https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#:~:text=The%20actual%20package%20name%20of%20the%20main%20CDK%20package%20varies%20by%20language.) 

3. The following packages are required and can be installed using the helper script [client-setup.sh](./client-setup.sh):
    * [Helm 3.8.2](https://helm.sh/)
    * [kubectl 1.21.0](https://kubernetes.io/docs/tasks/tools/#kubectl)
    * [JQ](https://stedolan.github.io/jq/download/)

4. Create an IAM user with the following permissions in the AWS account that you will be deploying this solution:
    * Administrator and Programmatic access
    * GitHub repository creation and write access

5. The [CDK code](./cdk/pipeline) deploys a code pipeline and automates the deployment of the components. The code pipeline requires a supported Git repository (GitHub or Bitbucket) as the source. In this example we are using GitHub. Follow these steps to create a GitHub repo:
    1. Create a new GitHub repo as a fork of the opensource. Follow the instructions [here](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo)
    2. Create an OAuth Personal Access Token with read access to the repository and read/write access to Webhooks. Follow instructions [here](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-fine-grained-personal-access-token).

6. Create a new secret in AWS Secrets Manager via the AWS Console to store the GitHub Personal Access Token (PAT) from the step above. Ensure you are in the correct region:
    1. Login to the AWS console and navigate to AWS Secrets Manager.
    2. Select `Store a new secret`, click `Other type of secret`, use the `plaintext` option and save the GitHub PAT from the step above. 
        >NOTE Don’t use the key value pair nor the Json format. 
    3. Enter a name for the secret. This name is required in the cdk context file later on.
    4. Leave the rest as default and store the secret.

### Service Limits (not required atm)

Increase the following service limits via Service Quotas section in AWS Console.
* DynamoDB table Read Capacity Unit (RCU) limit should be increased to 150,000 RCU's from default 40,000 RCU's. The limit is called `Table-level read throughput limit` under the `Amazon DynamoDB` service quota.
* Kinesis shards limit should be increased to 2048. The limit is called `Shard per Region` under `Amazon Kinesis Data Streams` service quota.

## Architecture Diagram

![Architecture](./images/RTBCodekitArchitecture.png)

## Deployment

1. Clone this repo to your local machine.
4. Navigate to `aws-real-time-bidder/cdk/pipeline` in your terminal or IDE and copy `cdk.context.json.example` file to `cdk.context.json`:
    ```
    cd cdk/pipeline
    cp cdk/pipeline/cdk.context.json.example cdk/pipeline/cdk.context.json
    ```

5. Configure your settings by creating a `.env` file in the root (use `envtemplate` as templte). Update the `STACK_NAME`,`STACK_VARIANT` (DynamoDB/Aerospike/DynamoDBBasic) and the rest of the variables in the `.env` file. 
**Important:** make sure STACK_NAME is unique as it creates a bucket with that name.
    ```
    cp envtemplate .env
    cat .env

    STACK_NAME=rtbkit-<your-user-alias>-<region>
    AWS_REGION=us-east-1
    STACK_VARIANT=DynamoDbBasic
    REPO_OWNER=<your GitHub handle>
    REPO_NAME=guidance-for-building-a-real-time-bidder-for-advertising-on-aws
    REPO_BRANCH=main
    TARGET_LOCAL="http://bidder/bidrequest"
    TARGET_HEIMDALL="https://<your rtb app>.<acct>.<region>.dataplane.rtb.mpofxdevmu.aws.dev/link/<link-id>/bidrequest"
    TARGET_PUBLIC_NLB="http://<your-nlb-dns>.amazonaws.com/bidrequest"
    TARGET=$(TARGET_HEIMDALL) # or set to $(TARGET_LOCAL)

    ```
6.  Check if python3 and the python3 virtual environment are installed on your machine if not install python3:
    ```
    python3 --version
    ```
7.  Set up CDK (installs the requirements and boto3 libraries and bootstraps):
    ```
    make cdk@setup
    # validate the settings
    make cdk@list
    ```
8. Deploy the CDK stack:
    ```
    make cdk@deploy
    ```
9. CDK will deploy the resources as shown below:

    ![CDK Deployment](./images/CDKDeployment-2.png)
    ![CDK Deployment](./images/CDKDeployment.png)

10. On successful deployment you will see as following
    ```
    ✅ RTBBuildStack
    ```

11. After the CDK deployment is complete, a CodeBuild project will be provisioned ready to build and deploy both infrastructure and the `Real-Time-Bidding Solution` in your AWS Account using CloudFormation. 

12. Kick off the CodeBuild build by running the following command:

```sh
cd ../.. # return to the project root
aws codebuild start-build --project-name "rtb-build-project"
```
This will take approximately twenty minutes. Once successful you will see:

    ![Build Success](./images/buildsuccess.png)

12. (Optional for Aerospike variant) Open AWS console and navigate to EKS service. Locate the cluster deployed by the stack. Navigate to Add-ons tab and install the Amazon EBS CSI Driver add on:
    > **IMPORTANT**: Select `Optional configuration schema` and click select `Override` for the `Conflict resolution method` 
    
    ![EKS Add on](./images/eks-addon.png)

13. Once the deployment is completed go to the CloudFormation console and navigate to root stack (this will be the stack with name that you have provided in cdk.json file in step 4). Go to Outputs tab and copy `ApplicationStackName`, `ApplicationStackARN`, `EKSAccessRoleARN`, `EKSWorkerRoleARN`. We will be using them in the following steps.

14. Ensure the pre-requisites (`Helm`, `Kubectl` and `jq`) are installed on the local/client machine as per the pre-requisites section above.
    > Tip: The pre-requisites can be installed using the shell script [client-setup.sh](./client-setup.sh). Navigate to the root directory and change the script permissions `chmod 700 client-setup.sh` before running the script.

    >NOTE: Commands for steps 15 - 22 are included in a shell script [run-benchmark.sh](./run-benchmark.sh). Navigate to the directory and change the script permissions `chmod 700 client-setup.sh` if required before running the script.

15. Now run the `make` command to access the EKS cluster by:
    >NOTE: This command has to be run from the root folder of the code repository:

    ```
    make eks@use
    ```

16. Run the following command to list the pods in cluster. You should see the pods as shown in the screenshot below.
    ```
    kubectl get pods
    ```

    ![Get Pods](./images/getpods.png)

17. The below command will clean up the existing load generator container that was deployed during the initial deployment. You need to run this command every time you want to run a new benchmark. Use the script [run-benchmark.sh](./run-benchmark.sh) that automates 21-22.
    ```
    make benchmark@cleanup
    ```
    
18. If you plan to run benchmarks using distributed setup proceed to section "How to use the RTB guidance with Heimdall". Start the benchmark by initiating the load-generator along with the parameters.
    ```
    make benchmark@run TIMEOUT=100ms NUMBER_OF_JOBS=1 RATE_PER_JOB=200 NUMBER_OF_DEVICES=10000 DURATION=500s
    ```
    _You can supply following parameters to load-generator and perform benchmarks_
    ```
    TIMEOUT=100ms        # Request timeout (default 100ms)
    DURATION=500s        # duration of the load generation
    RATE_PER_JOB=5000    # target request rate for the load generator
    NUMBER_OF_DEVICES=10 # number of devices IFAs to use in bid request
    NUMBER_OF_JOBS=1     # number of parallel instances of the load generator
    SLOPE=0              # slope of requests per second increase (zero for a constant rate; see <https://en.wikipedia.org/wiki/Slope>)
    ENABLE_PROFILER=1    # used to start profiling session, leave unset to disable
    ```

23. Once the load-generator is started you can run the following port-forward command to connect to Grafana Dashboard.
    ```
    kubectl port-forward svc/prom-grafana 8080:80
    ```

24. On your local/client instance open the URL [localhost:8080](http://localhost:8080) access Grafana Dashboard. 
Use the following credentials to login (Turn off enhanced tracking if you are using Firefox)

    ```
    username: admin
    Password: prom-operator
    ```

    ![Grafana login](./images/aws-rtb-grafana-login.png)

25. Once you login, click on the dashboard button on the left hamburger menu and select manage as shown in the figure below.

    ![Dashboards](./images/aws-rtb-grafana-dashboards.png)

26. Search and access 'bidder' dashboard from the list.

    ![Bidder](./images/aws-rtb-grafana-bidder.png)

27. You will see the bid request that are being generated on the right side and latency on the left side of the dashboard as shown in the figure below.

    ![Benchmark](./images/benchmarksresults.png)

The metrics include:

* Bid requests generated
* Bid requests received
* No Bid responses
* Latency on 99, 95 and 90 percentiles

These benchmarks help demonstrate the Real-time-bidder application performance on AWS Graviton instances.

>IMPORTANT :  After running the benchmark delete the stack to avoid incurring AWS cloud costs

>TIP: You can also change the DynamoDB and kinesis configurations to use on-demand compute instead of provisioned. This will reduce the cost of running the resources in the stack. In addition to this go to EKS and set the max and desired count of node in all node groups to 0. This will automatically terminate all the running EC2 instances launched by the stack. These two changes could reduce the running cost to 1/10th. As of June 2024, the average running cost of the stack with basic configuration is around $800/day in us-west-2

# Notes
* You can disable the data pipeline by setting KINESIS_DISABLE: "true" in deployment/infrastructure/charts/bidder/values.yaml file
* We have used unsafe pointers to optimize the heap allocation and are not converting the pointer types in the code. If this code is used in production, we strongly recommend you to look at your current code and set the pointer type in ./apps/bidder/code/stream/producer/batch_compiler.go file.
* For the ease of deployment, we have pre-configured user credentials for Grafana Dashboard. This is not a best practice and we strongly recommend you to configure access via AWS IAM/SSO. (./deployment/Infrastructure/deployment/prometheus/prometheus-values.yaml, ./tools/benchmark-utils/function.sh)
* Since CloudTrail is enabled on the AWS account by default. we strongly recommend not to disable it. We have not made any CloudTrail configuration on the code kit to enable it if it is disabled.
* Use [update-node-group-size.sh](./update-node-group-size) to downsize the EKS cluster to zero to save compute costs while testing
* Use [check-pod-logs.sh](./check-pod-logs.sh) to get the bidder logs for troubleshooting purposes

# How to use NLB with RTB Kit

1. Deploy NLB: `kubectl apply -f deployment/infrastructure/deployment/bidder-nlb.yaml`
2. Get DNS hostname of the loadbalancer: `kubectl get services bidder-nlb -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'`

# How to use the RTB guidance with Heimdall

## Provision publisher infrastructure. 

Your publisher infrastructure requires the following:

1. AWS Account (generally, it is a separate account from the bidder application)
2. VPC 
3. EKS Cluster to deploy the publisher app (emulated by the load generator in this solution)

We have prepared commands for you to accomplish steps #2 and #3. 

1. Start a new terminal session and navigate to the target account by setting AWS credentials and context to use it (e.g. switching AWS CLI profile). Starting a new session is required in order avoid context pollution from your previous runs. 

2. Run `make publisher-eks@provision` in the publisher account. This will provision a new VPC with a well-architected EKS cluster in Auto Mode. The cluster has an ARM Node Pool, managed Karpenter and a number of add-ons out of the box.

3. The above command will automatically update your .kube/config. You can validate access by running:
```
kubectl config current-context # should produce something like <username>@publisher-eks.us-east-1.eksctl.io
kubectl get nodepools
```

You will see nodepools `system` and `generic-workloads`. The latter is a pool based on Graviton instances to run load testing. 

4. Onboard the private subnets of the created VPC as part of your publisher application in Heimdall. 

```
aws eks describe-cluster --name publisher-eks --query "cluster.resourcesVpcConfig.subnetIds" --output text
```

Run `create-requester-rtb-app` (using onboarding guide).

5. In your `.env` file set variable `TARGET_HEIMDALL` to the link that you established with the responder application. Then set the `TARGET` to point to the `TARGET_HEIMDALL`.

```
TARGET_HEIMDALL="https://<rtb-app-id>.<acct>.<region>.dataplane.rtb.mpofxdevmu.aws.dev/link/<link-id>/bidrequest"
TARGET=$(TARGET_HEIMDALL)
```
